{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter\n",
    "  \n",
    "如何用DataParallel来使用多 GPU。  \n",
    "通过PyTorch使用多个GPU非常简单。你可以将模型放在一个 GPU：  \n",
    "```\n",
    "device = torch.device(\"cuda:0\")  \n",
    "model.to(device)  \n",
    "```\n",
    "然后，你可以复制所有的张量到 GPU：  \n",
    "```\n",
    "mytensor = my_tensor.to(device)'  \n",
    "```\n",
    "只是调用 my_tensor.to(device) 返回一个 my_tensor 新的复制在GPU上，而不是重写 my_tensor。你需要分配给他一个新的张量并且在 GPU 上使用这个张量。  \n",
    "在多 GPU 中执行前馈、后馈操作是非常自然的。尽管如此，PyTorch 默认只会使用一个GPU。通过使用 DataParallel 让模型并行运行，是很容易的。  \n",
    "```\n",
    "model = nn.DataParallel(model)'   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数\n",
    "input_size = 5\n",
    "output_size = 2\n",
    "batch_size = 30\n",
    "data_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "# 生成一个玩具数据。你只需要实现 getitem. \n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7808, -0.2692,  0.2331,  1.4260,  0.1552]) tensor([-0.0328, -0.2052,  0.1001,  0.1216,  0.5792]) tensor([ 0.1884, -0.6037, -0.1976,  0.3486, -0.5048]) tensor([-0.9775,  0.4831, -1.2960, -1.4174, -0.9343]) tensor([ 1.3104, -0.1092,  0.4479, -1.1926,  0.0422]) tensor([-0.6270, -0.8805, -1.3384, -0.6248, -0.5144]) tensor([-0.1781, -0.2820,  1.3492, -0.1941,  0.4501]) tensor([-0.7169,  0.6996,  0.3792,  0.3767, -0.0820]) tensor([ 1.8997, -0.7821, -0.8401,  0.4070,  1.4371]) tensor([ 1.3957, -0.6585,  3.1682,  0.9110, -1.1238]) tensor([-1.8214,  0.7353, -1.0977, -0.3508,  0.4149]) tensor([-0.9801, -0.0915, -0.1385, -0.6687, -0.6169]) tensor([-0.0890,  0.0896,  0.5461,  0.1845,  1.9233]) tensor([-1.0313, -0.2603,  0.7342,  1.5796, -2.2195]) tensor([-0.5531,  1.0040, -1.0429,  0.6244, -0.0396]) tensor([ 0.3229,  0.4049, -2.2026,  0.3023, -0.4129]) tensor([ 1.3366,  1.4404, -1.5362, -1.0688, -0.9580]) tensor([ 0.6549,  1.1600, -1.2970, -0.2273,  2.8085]) tensor([ 0.2425, -1.5962, -0.4255,  1.7994,  1.4246]) tensor([-0.6905,  0.6943, -0.0527,  0.2498,  0.5634]) tensor([ 0.0069,  1.4519,  2.5979, -1.5134, -0.3695]) tensor([-0.8703, -0.5409, -0.5184, -0.2342,  0.4658]) tensor([ 0.7089, -0.3908,  0.6733, -1.0371,  1.9731]) tensor([-1.2541,  0.2230, -0.8680,  0.9544, -0.0260]) tensor([ 0.0325,  0.0458,  1.3147, -1.2243, -0.8438]) tensor([-0.5424, -0.2631,  0.0622, -0.7379,  0.5524]) tensor([-1.5996, -1.3776, -0.8742, -0.1116,  0.7216]) tensor([-0.1620,  1.6486, -0.5050,  0.4178, -1.1903]) tensor([-0.6584,  0.2507,  1.3155,  0.7762,  0.8276]) tensor([ 0.7835,  0.1388, -0.1476, -0.4050, -0.6023]) tensor([-0.9175,  0.4922,  0.6716, -0.3036, -0.6906]) tensor([ 0.3916, -0.0826,  0.3168, -0.4585, -0.9186]) tensor([-2.4134, -1.2140,  0.9675,  0.2211, -0.1859]) tensor([ 0.6149,  0.2392, -0.6656,  0.9416, -0.1270]) tensor([ 1.4161,  0.1947,  1.1486, -0.3876,  0.1327]) tensor([ 0.8395, -1.4117, -0.1704,  0.9437,  0.2123]) tensor([1.0880, 0.6608, 2.2429, 0.4229, 0.1063]) tensor([-0.4699,  0.8886,  1.3555,  1.0349,  0.0532]) tensor([-1.0181,  1.0819, -0.1822,  0.3816, -0.2518]) tensor([-0.6984,  1.8285,  0.2999,  0.6654, -1.5067]) tensor([ 0.0354,  0.4207,  1.2992,  0.3708, -0.4617]) tensor([-0.3140, -1.0146, -1.1140, -2.6134, -2.2671]) tensor([ 0.2368, -1.1872,  0.2752,  1.0491,  1.4714]) tensor([ 1.5105,  0.7062, -0.5222, -1.1746,  1.3550]) tensor([ 0.0626,  1.2169, -1.1329, -1.3548, -0.0293]) tensor([ 1.1995, -1.1054, -2.6773, -0.9555, -1.2405]) tensor([ 0.9750,  0.6600, -1.0836, -0.4814, -1.4450]) tensor([ 0.3105,  0.1124, -0.6985, -1.7818, -0.1743]) tensor([ 0.3860, -0.8842,  0.7960, -0.0699, -0.4643]) tensor([ 1.4856, -1.1436, -0.0763, -0.0541, -0.5931]) tensor([ 0.1976, -2.4709,  0.1618, -1.5245,  0.4281]) tensor([-1.3717,  0.3500, -1.5914, -0.7392,  0.5382]) tensor([-0.3618, -0.5116,  0.1143,  2.5303, -0.9475]) tensor([ 1.1770,  0.5440, -0.1252,  0.8912, -0.6928]) tensor([ 0.5973,  1.6210,  0.0264, -0.1457,  0.0605]) tensor([ 0.8708, -2.5248,  0.4774,  0.3532, -1.1968]) tensor([-0.2295,  0.2572,  1.2929,  0.8212,  0.2411]) tensor([-0.3792,  1.1366,  1.7921,  0.2616,  1.4993]) tensor([ 0.4425,  0.7360, -0.4864,  0.6176, -0.5802]) tensor([-0.0174,  0.1409, -0.7463,  0.3059,  0.3654]) tensor([ 0.5364,  0.5206,  1.9514, -0.3758,  1.3058]) tensor([ 0.1194, -1.1265,  0.9737,  0.6313,  1.4302]) tensor([-0.8492,  0.9199, -0.2755, -0.5099, -0.3634]) tensor([-0.6570, -0.6166,  1.8040,  1.2338, -1.0793]) tensor([-0.2589, -1.7206, -0.6804,  1.9153,  0.2625]) tensor([-1.9339,  0.1553,  0.1531,  0.3366, -1.9235]) tensor([ 5.3445e-01,  4.8396e-01, -1.4895e+00,  3.9912e-04,  7.4781e-01]) tensor([-0.4401, -0.1322,  1.2185,  1.9643, -0.5240]) tensor([-0.9604,  0.0221, -0.2931,  0.0066, -0.2656]) tensor([ 0.7695, -0.4611,  0.2873, -0.4813, -0.7339]) tensor([-2.0072,  1.0821, -1.8905,  0.1279,  0.1138]) tensor([-0.8629, -0.7952,  0.2786,  0.6208, -0.4895]) tensor([-0.3664, -0.9181, -0.0557,  1.0436, -0.7336]) tensor([0.9988, 0.3170, 0.7643, 1.3821, 0.1662]) tensor([ 0.8749,  0.4858,  0.7085,  0.5644, -0.3341]) tensor([ 0.1072, -0.9206, -0.7707, -0.2085, -1.4352]) tensor([-0.1520, -0.8802, -0.0462, -0.2244,  0.3886]) tensor([ 1.0399,  0.2417, -0.0612, -0.2271, -0.4458]) tensor([ 0.7537, -0.4728, -0.1087, -1.2396, -0.3264]) tensor([ 1.8165,  0.3864, -1.9144,  1.5250,  0.7337]) tensor([ 0.0696,  0.8885, -1.4418,  0.4954,  0.3272]) tensor([-0.7732, -0.8287, -1.2908, -1.1197, -1.7823]) tensor([ 1.2240, -0.0051, -0.0736,  0.1166, -0.0085]) tensor([-0.7663,  0.6313,  1.2396, -1.5086, -0.9833]) tensor([ 1.6813,  0.0975, -0.4770,  0.4100, -0.2690]) tensor([ 0.2490, -0.6834, -3.5339, -1.3686,  1.1079]) tensor([-0.2323, -0.1648,  0.3512, -1.0683,  0.5324]) tensor([-1.1759, -0.8328, -0.0313,  1.6293,  1.0649]) tensor([-1.3893,  0.3701, -0.3190,  0.0542,  1.7494]) tensor([-1.1766, -1.0532,  1.1509,  0.8850, -0.1041]) tensor([-0.5241,  1.6439,  0.7248, -1.4332,  0.4191]) tensor([-0.9966,  0.4548, -0.7772,  0.4275,  2.1930]) tensor([ 1.3229,  1.0412,  0.0040,  0.1616, -1.4254]) tensor([ 1.0597, -1.2201, -1.6713,  0.0208, -0.7630]) tensor([ 1.6718, -1.3297, -1.6110,  0.5589,  1.6732]) tensor([ 0.3073, -1.9817,  1.2096, -0.4613,  2.1333]) tensor([-0.8907, -0.4879,  0.6476,  0.8782, -0.3110]) tensor([-0.2896, -1.4146,  0.4809,  0.8787,  0.3011]) tensor([-0.0902, -0.1068, -1.8887,  1.4424,  0.8775]) tensor([ 0.9454,  2.3740, -0.1650,  0.9925, -0.2057])\n"
     ]
    }
   ],
   "source": [
    "print (*RandomDataset(input_size, data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_loader = DataLoader(dataset = RandomDataset(input_size, data_size), \n",
    "                        batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单模型\n",
    "# 的模型只是获得一个输入，执行一个线性操作，然后给一个输出。\n",
    "# 放置了一个输出声明在模型中来检测输出和输入张量的大小。\n",
    "# 注意在 batch rank 0 中的输出。\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.fc(input)\n",
    "        print(\"\\tIn Model: input size\", input.size(),\"output size\", output.size())\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "cnt = torch.cuda.device_count()\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(input_size, output_size)\n",
    "if cnt > 1:\n",
    "    print(\"Let's use\", cnt, \"GPU\")\n",
    "    model = nn.DataParallel(model) # 重点⭐\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
      "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
      "\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
      "Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "for data in rand_loader:\n",
    "    input = data.to(device)\n",
    "    output = model(input)\n",
    "    print(\"Outside: input size\", input.size(),\"output_size\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据并行自动拆分了数据并且将任务单发送到多个 GPU 上。当每一个模型都完成自己的任务之后，DataParallel 收集并且合并这些结果然后返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
